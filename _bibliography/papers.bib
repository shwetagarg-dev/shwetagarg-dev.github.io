---
---

@inproceedings{gupta2022mitigating,
  title={Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal},
  author={Gupta, Umang and Dhamala, Jwala and Kumar, Varun and Verma, Apurv and Pruksachatkun, Yada and Krishna, Satyapriya and Gupta, Rahul and Chang, Kai-Wei and Ver Steeg, Greg and Galstyan, Aram},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={658--678},
  year={2022}
}

@inproceedings{cao2022intrinsic,
  title={On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations},
  author={Cao, Yang Trista and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul and Kumar, Varun and Dhamala, Jwala and Galstyan, Aram},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={561--570},
  year={2022}
}

@inproceedings{ding-etal-2023-static,
    title = "A Static Evaluation of Code Completion by Large Language Models",
    author = "Ding, Hantian  and
      Kumar, Varun  and
      Tian, Yuchen  and
      Wang, Zijian  and
      Kwiatkowski, Rob  and
      Li, Xiaopeng  and
      Ramanathan, Murali Krishna  and
      Ray, Baishakhi  and
      Bhatia, Parminder  and
      Sengupta, Sudipta",
    editor = "Sitaram, Sunayana  and
      Beigman Klebanov, Beata  and
      Williams, Jason D",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-industry.34",
    doi = "10.18653/v1/2023.acl-industry.34",
    pages = "347--360",
    abstract = "Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the other hand, static analysis tools such as linters, which can detect errors without running the program, haven{'}t been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models. Through extensive studies, we also show the impact of sampling temperature, model size, and context on static errors in code completions.",
}

@inproceedings{wang-etal-2023-recode,
    title = "{R}e{C}ode: Robustness Evaluation of Code Generation Models",
    author = "Wang, Shiqi  and
      Li, Zheng  and
      Qian, Haifeng  and
      Yang, Chenghao  and
      Wang, Zijian  and
      Shang, Mingyue  and
      Kumar, Varun  and
      Tan, Samson  and
      Ray, Baishakhi  and
      Bhatia, Parminder  and
      Nallapati, Ramesh  and
      Ramanathan, Murali Krishna  and
      Roth, Dan  and
      Xiang, Bing",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.773",
    doi = "10.18653/v1/2023.acl-long.773",
    pages = "13818--13843",
    abstract = "Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model{'}s robustness performance. With human annotators, we verified that over 90{\%} of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.",
}

@inproceedings{mehrabi-etal-2023-resolving,
    title = "Resolving Ambiguities in Text-to-Image Generative Models",
    author = "Mehrabi, Ninareh  and
      Goyal, Palash  and
      Verma, Apurv  and
      Dhamala, Jwala  and
      Kumar, Varun  and
      Hu, Qian  and
      Chang, Kai-Wei  and
      Zemel, Richard  and
      Galstyan, Aram  and
      Gupta, Rahul",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.804",
    doi = "10.18653/v1/2023.acl-long.804",
    pages = "14367--14388",
    abstract = "Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for machines. In this work, we study ambiguities that arise in text-to-image generative models. We curate the Text-to-image Ambiguity Benchmark (TAB) dataset to study different types of ambiguities in text-to-image generative models. We then propose the Text-to-ImagE Disambiguation (TIED) framework to disambiguate the prompts given to the text-to-image generative models by soliciting clarifications from the end user. Through automatic and human evaluations, we show the effectiveness of our framework in generating more faithful images aligned with end user intention in the presence of ambiguities.",
}

@inproceedings{athiwaratkun2022multi,
  title={Multi-lingual Evaluation of Code Generation Models},
  author={Athiwaratkun, Ben and Gouda, Sanjay Krishna and Wang, Zijian and Li, Xiaopeng and Tian, Yuchen and Tan, Ming and Ahmad, Wasi Uddin and Wang, Shiqi and Sun, Qing and Shang, Mingyue and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{dhamala2023analysis,
  title={An analysis of the effects of decoding algorithms on fairness in open-ended language generation},
  author={Dhamala, Jwala and Kumar, Varun and Gupta, Rahul and Chang, Kai-Wei and Galstyan, Aram},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)},
  pages={655--662},
  year={2023},
  organization={IEEE}
}

@inproceedings{kumar-etal-2020-data,
    title = "Data Augmentation using Pre-trained Transformer Models",
    author = "Kumar, Varun  and
      Choudhary, Ashutosh  and
      Cho, Eunah",
    booktitle = "Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.lifelongnlp-1.3",
    pages = "18--26",
    selected={true},
    abstract = "Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information.",
}

@inproceedings{chen-etal-2021-industry,
    title = "Industry Scale Semi-Supervised Learning for Natural Language Understanding",
    author = "Chen, Luoxin  and
      Garcia, Francisco  and
      Kumar, Varun  and
      Xie, He  and
      Lu, Jianhua",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-industry.39",
    doi = "10.18653/v1/2021.naacl-industry.39",
    pages = "311--318",
    abstract = "This paper presents a production Semi-Supervised Learning (SSL) pipeline based on the student-teacher framework, which leverages millions of unlabeled examples to improve Natural Language Understanding (NLU) tasks. We investigate two questions related to the use of unlabeled data in production SSL context: 1) how to select samples from a huge unlabeled data pool that are beneficial for SSL training, and 2) how does the selected data affect the performance of different state-of-the-art SSL techniques. We compare four widely used SSL techniques, Pseudo-label (PL), Knowledge Distillation (KD), Virtual Adversarial Training (VAT) and Cross-View Training (CVT) in conjunction with two data selection methods including committee-based selection and submodular optimization based selection. We further examine the benefits and drawbacks of these techniques when applied to intent classification (IC) and named entity recognition (NER) tasks, and provide guidelines specifying when each of these methods might be beneficial to improve large scale NLU systems.",
}


@inproceedings{kumar-etal-2019-closer,
    title = "A Closer Look At Feature Space Data Augmentation For Few-Shot Intent Classification",
    author = "Kumar, Varun  and
      Glaude, Hadrien  and
      de Lichy, Cyprien  and
      Campbell, Wlliam",
    booktitle = "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6101",
    doi = "10.18653/v1/D19-6101",
    pages = "1--10",
    abstract = "New conversation topics and functionalities are constantly being added to conversational AI agents like Amazon Alexa and Apple Siri. As data collection and annotation is not scalable and is often costly, only a handful of examples for the new functionalities are available, which results in poor generalization performance. We formulate it as a Few-Shot Integration (FSI) problem where a few examples are used to introduce a new intent. In this paper, we study six feature space data augmentation methods to improve classification performance in FSI setting in combination with both supervised and unsupervised representation learning methods such as BERT. Through realistic experiments on two public conversational datasets, SNIPS, and the Facebook Dialog corpus, we show that data augmentation in feature space provides an effective way to improve intent classification performance in few-shot setting beyond traditional transfer learning approaches. In particular, we show that (a) upsampling in latent space is a competitive baseline for feature space augmentation (b) adding the difference between two examples to a new example is a simple yet effective data augmentation method.",
}

@inproceedings{kumar-etal-2019-didnt,
    title = "Why Didn{'}t You Listen to Me? Comparing User Control of Human-in-the-Loop Topic Models",
    author = "Kumar, Varun  and
      Smith-Renner, Alison  and
      Findlater, Leah  and
      Seppi, Kevin  and
      Boyd-Graber, Jordan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1637",
    doi = "10.18653/v1/P19-1637",
    pages = "6323--6330",
    abstract = "To address the lack of comparative evaluation of Human-in-the-Loop Topic Modeling (HLTM) systems, we implement and evaluate three contrasting HLTM modeling approaches using simulation experiments. These approaches extend previously proposed frameworks, including constraints and informed prior-based methods. Users should have a sense of control in HLTM systems, so we propose a control metric to measure whether refinement operations{'} results match users{'} expectations. Informed prior-based methods provide better control than constraints, but constraints yield higher quality topics.",
}

@inproceedings{zirikly-etal-2016-gw,
    title = "The {GW}/{UMD} {CLP}sych 2016 Shared Task System",
    author = "Zirikly, Ayah  and
      Kumar, Varun  and
      Resnik, Philip",
    booktitle = "Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology",
    month = jun,
    year = "2016",
    address = "San Diego, CA, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-0321",
    doi = "10.18653/v1/W16-0321",
    pages = "166--170",
}
@inproceedings{10.1145/3172944.3172965,
author = {Smith, Alison and Kumar, Varun and Boyd-Graber, Jordan and Seppi, Kevin and Findlater, Leah},
title = {Closing the Loop: User-Centered Design and Evaluation of a Human-in-the-Loop Topic Modeling System},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172965},
doi = {10.1145/3172944.3172965},
abstract = {Human-in-the-loop topic modeling allows users to guide the creation of topic models
and to improve model quality without having to be experts in topic modeling algorithms.
Prior work in this area has focused either on algorithmic implementation without understanding
how users actually wish to improve the model or on user needs but without the context
of a fully interactive system. To address this disconnect, we implemented a set of
model refinements requested by users in prior work and conducted a study with twelve
non-expert participants to examine how end users are affected by issues that arise
with a fully interactive, user-centered system. As these issues mirror those identified
in interactive machine learning more broadly, such as unpredictability, latency, and
trust, we also examined interactive machine learning challenges with non-expert end
users through the lens of human-in-the-loop topic modeling. We found that although
users experience unpredictability, their reactions vary from positive to negative,
and, surprisingly, we did not find any cases of distrust, but instead noted instances
where users perhaps trusted the system too much or had too little confidence in themselves.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {293–304},
numpages = {12},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3377325.3377491,
author = {Smith-Renner, Alison and Kumar, Varun and Boyd-Graber, Jordan and Seppi, Kevin and Findlater, Leah},
title = {Digging into User Control: Perceptions of Adherence and Instability in Transparent Models},
year = {2020},
isbn = {9781450371186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377325.3377491},
doi = {10.1145/3377325.3377491},
abstract = {We explore predictability and control in interactive systems where controls are easy
to validate. Human-in-the-loop techniques allow users to guide unsupervised algorithms
by exposing and supporting interaction with underlying model representations, increasing
transparency and promising fine-grained control. However, these models must balance
user input and the underlying data, meaning they sometimes update slowly, poorly,
or unpredictably---either by not incorporating user input as expected (adherence)
or by making other unexpected changes (instability). While prior work exposes model
internals and supports user feedback, less attention has been paid to users' reactions
when transparent models limit control. Focusing on interactive topic models, we explore
user perceptions of control using a study where 100 participants organize documents
with one of three distinct topic modeling approaches. These approaches incorporate
input differently, resulting in varied adherence, stability, update speeds, and model
quality. Participants disliked slow updates most, followed by lack of adherence. Instability
was polarizing: some participants liked it when it surfaced interesting information,
while others did not. Across modeling approaches, participants differed only in whether
they noticed adherence.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {519–530},
numpages = {12},
keywords = {topic modeling, interactive machine learning, intelligent user interface evaluation, control, transparency},
location = {Cagliari, Italy},
series = {IUI '20}
}

@INPROCEEDINGS{9003747,  author={Cho, Eunah and Xie, He and Lalor, John P. and Kumar, Varun and Campbell, William M.},  booktitle={2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},   title={Efficient Semi-Supervised Learning for Natural Language Understanding by Optimizing Diversity},   year={2019},  volume={},  number={},  pages={1077-1084},  doi={10.1109/ASRU46091.2019.9003747}}


@inproceedings{10.1145/2818052.2869096,
author = {Kumar, Varun and Pedanekar, Niranjan},
title = {Mining Shapes of Expertise in Online Social Q&amp;A Communities},
year = {2016},
isbn = {9781450339506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818052.2869096},
doi = {10.1145/2818052.2869096},
abstract = {Expertise of an individual is metaphorically defined by shapes of letters such as
I, T, M and hyphen, depending on her expertise in an area (depth) and the number of
areas of interest (width). Industries have now started recruiting people with specific
shapes of expertise. In this poster, we introduce the idea of mining shapes of user
expertise in a typical online social Question and Answer (Q&amp;A) community where expert
users often answer questions posed by other users. We report observations on distribution
of different shapes of expertise in a StackExchange community called Super User.},
booktitle = {Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion},
pages = {317–320},
numpages = {4},
keywords = {Online Social Q&amp;A, Shapes of Expertise, Expertise Profiling, A Communities, Graph Mining},
location = {San Francisco, California, USA},
series = {CSCW '16 Companion}
}

@inproceedings{10.1145/3442188.3445924,
author = {Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
title = {BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445924},
doi = {10.1145/3442188.3445924},
selected={true},
abstract = {Recent advances in deep learning techniques have enabled machines to generate cohesive
open-ended text when prompted with a sequence of words as context. While these models
now empower many downstream applications from conversation bots to automatic storytelling,
they have been shown to generate texts that exhibit social biases. To systematically
study and benchmark social biases in open-ended language generation, we introduce
the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that
consists of 23,679 English text generation prompts for bias benchmarking across five
domains: profession, gender, race, religion, and political ideology. We also propose
new automated metrics for toxicity, psycholinguistic norms, and text gender polarity
to measure social biases in open-ended text generation from multiple angles. An examination
of text generated from three popular language models reveals that the majority of
these models exhibit a larger social bias than human-written Wikipedia text across
all domains. With these results we highlight the need to benchmark biases in open-ended
language generation and caution users of language generation models on downstream
tasks to be cognizant of these embedded prejudices.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {862–872},
numpages = {11},
keywords = {natural language generation, Fairness},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@INPROCEEDINGS{9383495,  author={Kumar, Manoj and Kumar, Varun and Glaude, Hadrien and de Lichy, Cyprien and Alok, Aman and Gupta, Rahul},  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},   title={Protoda: Efficient Transfer Learning for Few-Shot Intent Classification},   year={2021},  volume={},  number={},  pages={966-972},  doi={10.1109/SLT48900.2021.9383495}}
